{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data identification and acquisition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful utilities\n",
    "Since problems will happen, we need to be able to handle them in a nice and clean way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete duplicates\n",
    "This script deletes all the duplicate lines from `gigs.txt` and can be easily modified for `categories.txt`.\n",
    "Notice that unlike post-processing, this script **can make the time require to scrap** faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gigs.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines_set = set(lines)\n",
    "lines_set = sorted(lines_set)\n",
    "\n",
    "with open('gigs.txt', 'w') as f:\n",
    "    f.writelines(lines_set)\n",
    "\n",
    "print('Deleted ' + str(len(lines) - len(lines_set)) + ' duplicate lines.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversing the order of the lines\n",
    "If two users wants to scrap the same data, they can do it in parallel by reversing the order of the lines in `gigs.txt` and `categories.txt`.\n",
    "this can be easily done by appending `[::-1]` to the end of of every list that is being enumerated in the `#Crawling` section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium_stealth import stealth\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import threading\n",
    "import os\n",
    "import sys\n",
    "import traceback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize stealthy Chrome driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "chrome_driver_path = r\"C:\\Users\\Deftera\\Downloads\\chromedriver_win32\\chromedriver.exe\"\n",
    "s = Service(chrome_driver_path)\n",
    "\n",
    "driver = webdriver.Chrome(service=s, options=options, service_args=[\"--verbose\"])\n",
    "\n",
    "stealth(driver,\n",
    "      languages=[\"en-US\", \"en\"],\n",
    "      vendor=\"Google Inc.\",\n",
    "      platform=\"Win32\",\n",
    "      webgl_vendor=\"Intel Inc.\",\n",
    "      renderer=\"Intel Iris OpenGL Engine\",\n",
    "fix_hairline=True,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the timeout_callback function\n",
    "Since the data is being scraped using selenium_stealth, which is very unstable, the driver can get stuck at any point of time. To avoid this, we use a timeout_callback function which will be called if the driver gets stuck. This function will then `taskkill` the driver and the current process. (**Note:** This function is neccessary to continue the scraping process even if the driver gets stuck, a `.bat` file will be responsible for restarting the process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeout_callback(category_url):\n",
    "    os.system(\"taskkill /F /IM chrome.exe /T\") \n",
    "    sys.exit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the program is running correctly, extract the jupyter notebook as a `.py` file, and run it with the following batch script.\n",
    "\n",
    "`gigs.txt` is a file containing the links of all the gigs to be scraped. The links are separated by a newline character.\n",
    "A similar file layout and batch script should be used for the category links as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "@echo off\n",
    "\n",
    "set \"gigs_file=gigs.txt\"\n",
    "set \"python_script=get_gigs.py\"\n",
    "set \"timeout_seconds=5\"\n",
    "\n",
    ":loop\n",
    "set /p first_line=<\"%gigs_file%\"\n",
    "timeout /t %timeout_seconds% /nobreak >nul\n",
    "python \"%python_script%\"\n",
    "\n",
    "set /p new_first_line=<\"%gigs_file%\"\n",
    "if \"%new_first_line%\"==\"%first_line%\" (\n",
    "    call :removeDuplicateLine\n",
    ")\n",
    "\n",
    "goto loop\n",
    "\n",
    ":removeDuplicateLine\n",
    "ren \"%gigs_file%\" \"gigs_temp.txt\"\n",
    "for /f \"skip=1 delims=\" %%a in (gigs_temp.txt) do echo %%a>>\"%gigs_file%\"\n",
    "del \"gigs_temp.txt\"\n",
    "exit /b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the crawling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_category(category_url, gigs_limit=15):\n",
    "    \"\"\"\n",
    "    Crawls a given category url and returns a list of the URLs of the gigs on the page\n",
    "    :param category_url: the url of the category to crawl\n",
    "    :param gigs_limit: the maximum number of gigs to return\n",
    "    :return: a list of gigs URLs with length of up to gigs_limit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set a timer to kill the process if it takes too long\n",
    "    timer = threading.Timer(18 , timeout_callback, [category_url])\n",
    "    timer.start()\n",
    "    try:\n",
    "        print(\"trying to reach \" + category_url)\n",
    "        driver.get(category_url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        gigs = soup.select(\"div.gig-card-layout > div.gig-wrapper > div.basic-gig-card > a\")\n",
    "        if len(gigs) == 0:\n",
    "            # If no gigs were found, stop the timer and return an empty list\n",
    "            timer.cancel()\n",
    "            return []\n",
    "        gigs_links = [gig.get(\"href\") for gig in gigs]\n",
    "        if len(gigs_links) == 0:\n",
    "            # if no gig *links* were found, stop the timer and return an empty list\n",
    "            timer.cancel()\n",
    "            return []\n",
    "        full_gigs_links = [f\"https://www.fiverr.com{link}\" for link in gigs_links] # add the domain to the links\n",
    "        full_gigs_links = list(set(full_gigs_links)) # remove duplicates\n",
    "        timer.cancel()\n",
    "    except Exception as e:\n",
    "        sys.exit() # kill the process if an exception is raised\n",
    "    return full_gigs_links[:gigs_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_gig(gig_url):\n",
    "    \"\"\"\n",
    "    Crawls a given gig url and returns a dictionary of the gig's data\n",
    "    :param gig_url: the url of the gig to crawl\n",
    "    :return: a dictionary of the gig's data \n",
    "    (title, rating score, orders in queue, rating counts, seller level, category, delivery times, prices,\n",
    "    revisions, tags, language, country, member since and features)\n",
    "    \"\"\"\n",
    "\n",
    "    # Set a timer to kill the process if it takes too long\n",
    "    timer = threading.Timer(18, timeout_callback, [gig_url])\n",
    "    timer.start()\n",
    "    try:\n",
    "        print(\"trying to reach \" + gig_url)\n",
    "        driver.get(gig_url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # load the gig's overview\n",
    "        gig_overview = soup.select_one(\n",
    "            \"div.main-content > div#__ZONE__main > div.gig_page_perseus > div.gig-page-wrapper > div.gig-page > div.main > div.gig-overview\"\n",
    "        )\n",
    "        gig_title = gig_overview.select_one(\"h1\").text\n",
    "        seller_overview = gig_overview.select_one(\"div.seller-overview\")\n",
    "        try:\n",
    "            rating_score = seller_overview.select_one(\"b.rating-score\").text\n",
    "        except:\n",
    "            # if no rating score was found, set it to 0\n",
    "            rating_score = 0\n",
    "        try:\n",
    "            seller_level = seller_overview.select_one(\n",
    "                \"div.Waqjn3u > div.user-profile-image\"\n",
    "            ).text\n",
    "        except:\n",
    "            # Sometimes the seller level is in another place, so if the first selector didn't work, try another one\n",
    "            seller_level = soup.select_one(\n",
    "                \"span.level > div.jEW3B9z > span.HsyURQF\"\n",
    "            ).text\n",
    "        try:\n",
    "            rating_counts = (\n",
    "                seller_overview.select_one(\"span.ratings-count\")\n",
    "                .text.replace(\"(\", \"\")\n",
    "                .replace(\")\", \"\")\n",
    "            )\n",
    "        except:\n",
    "            # if no rating counts were found, set it to 0\n",
    "            # also, it means that the seller is new, so set the seller level to 0\n",
    "            rating_counts = 0\n",
    "            seller_level = 0\n",
    "        try:\n",
    "            orders_in_queue = seller_overview.select_one(\"div.sfNimsX\").text.split(\" \")[\n",
    "                0\n",
    "            ]\n",
    "        except:\n",
    "            # if no orders in queue were found, set it to 0\n",
    "            orders_in_queue = 0\n",
    "        try:\n",
    "            # Get the gig's package details\n",
    "            packages_table = soup.select_one(\"div.gig-page-packages-table\")\n",
    "            delivery_times = packages_table.select_one(\"tr.delivery-time\").select(\"td\")[\n",
    "                1:\n",
    "            ]\n",
    "            for index, delivery_time in enumerate(delivery_times):\n",
    "                first_span = delivery_time.select_one(\"span:not([class])\")\n",
    "                if first_span:\n",
    "                    delivery_times[index] = first_span.text\n",
    "                else:\n",
    "                    delivery_times[index] = delivery_time.text\n",
    "\n",
    "            features = packages_table.select(\"tr.description > td\")\n",
    "            features = features[1:]\n",
    "            for i in range(len(features)):\n",
    "                features[i] = features[i].text\n",
    "\n",
    "            prices = packages_table.select_one(\"tr.select-package\").select(\n",
    "                \"td > div.price-wrapper > p\"\n",
    "            )\n",
    "            prices = [price.text for price in prices]\n",
    "            try:\n",
    "                revisions = packages_table.select(\"tr:not([class])\")[-1].select(\"td\")[\n",
    "                    1:\n",
    "                ]\n",
    "                revisions = [revision.text for revision in revisions]\n",
    "            except:\n",
    "                revisions = []\n",
    "\n",
    "            tags = soup.select(\"div.gig-tags-container > ul > li > a\")\n",
    "            extracted_tags = []\n",
    "            for tag in tags:\n",
    "                extracted_tags.append(str(tag).split(\">\")[1][:-4])\n",
    "\n",
    "            country = soup.select(\n",
    "                \"div.profile-card > div.seller-card > div.stats-desc > ul.user-stats > li > strong\"\n",
    "            )\n",
    "            member_since = str(country[1]).split(\">\")[1][:-8]\n",
    "            country = str(country[0]).split(\">\")[1][:-8]\n",
    "            language = soup.select_one(\"span.HsyURQF > strong\").text\n",
    "\n",
    "            category = soup.select(\"span.category-breadcrumbs\")[-1].select_one(\"a\").text\n",
    "        except:\n",
    "            # sometimes gig's package details are not available, so set them to None\n",
    "            # because their data won't help us in our analysis\n",
    "            timer.cancel()\n",
    "            return None\n",
    "        timer.cancel()\n",
    "    except Exception as e:\n",
    "        # if an exception is raised, kill the process\n",
    "        sys.exit()\n",
    "    return {\n",
    "        \"gig_title\": gig_title,\n",
    "        \"rating_score\": rating_score,\n",
    "        \"orders_in_queue\": orders_in_queue,\n",
    "        \"rating_counts\": rating_counts,\n",
    "        \"seller_level\": seller_level,\n",
    "        \"category\": category,\n",
    "        \"delivery_times\": delivery_times,\n",
    "        \"prices\": prices,\n",
    "        \"revisions\": revisions,\n",
    "        \"tags\": extracted_tags,\n",
    "        \"language\": language,\n",
    "        \"country\": country,\n",
    "        \"member_since\": member_since,\n",
    "        \"features\": features,\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling the category links\n",
    "Because we want to take data from all the categories, we first need to get the links of all the categories. This is done by the following function that crawls the category links and stores them in a file called `categories.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('categories.txt', 'r') as f:\n",
    "        full_categories_links = f.read().splitlines()\n",
    "except:\n",
    "    CATEGORIES_URL = \"https://www.fiverr.com/categories\"\n",
    "    driver.get(CATEGORIES_URL)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    categories = soup.select(\"section.mp-categories-columns.cf > ul > li > a\")\n",
    "    categories_links = [category.get(\"href\") for category in categories]\n",
    "    full_categories_links = [f\"https://www.fiverr.com{link}\" for link in categories_links]\n",
    "\n",
    "    with open('categories.txt', 'w') as f:\n",
    "        for item in full_categories_links:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "try:\n",
    "    with open('gigs.txt', 'r') as f:\n",
    "        gigs_links = f.read().splitlines()\n",
    "except:\n",
    "    gigs_links = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling the gig links from the category links\n",
    "Notice that the links are constantly being appended to the `gigs.txt` file. This is because the driver can get stuck at any point of time, and we don't want to lose the links that have already been crawled. So, with the help of the `.bat` file, we can restart the process from the last link that was crawled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_url in full_categories_links: # If reading in reverse order, you might wanna use full_categories_links[::-1] instead\n",
    "    if category_url in gigs_links:\n",
    "        continue\n",
    "    pprint(\"created timer\")\n",
    "    gigs = crawl_category(category_url)\n",
    "    \n",
    "    gigs_links += gigs\n",
    "    with open('gigs.txt', 'a') as f:\n",
    "        for item in gigs:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    with open('categories.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open('categories.txt', 'w') as f:\n",
    "        for line in lines:\n",
    "            if line.strip(\"\\n\") != category_url:\n",
    "                f.write(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling the gig data from the gig links\n",
    "Notice that the same script is being used to crawl the gig data as well. This is because the driver can get stuck at any point of time, and we don't want to lose the data that has already been crawled. So, with the help of the `.bat` file, we can restart the process from the last link that was crawled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('gigs_data.csv', 'r') as f:\n",
    "        pass\n",
    "except:\n",
    "    with open('gigs_data.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"gig_title\", \"rating_score\", \"orders_in_queue\", \"rating_counts\", \"seller_level\", \"category\", \"delivery_times\", \"prices\", \"revisions\", \"tags\", \"language\", \"country\", \"member_since\", \"features\"])\n",
    "\n",
    "for gig_link in gigs_links:\n",
    "    gig_data = crawl_gig(gig_link)\n",
    "    if gig_data == None:\n",
    "        with open('gigs.txt', 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        with open('gigs.txt', 'w') as f:\n",
    "            for line in lines:\n",
    "                if line.strip(\"\\n\") != gig_link:\n",
    "                    f.write(line)\n",
    "        continue\n",
    "\n",
    "    # since we are dealing with hebrew, we need to encode the data to utf-8\n",
    "    with open('gigs_data.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([gig_data[\"gig_title\"], gig_data[\"rating_score\"], gig_data[\"orders_in_queue\"], gig_data[\"rating_counts\"], gig_data[\"seller_level\"], gig_data[\"category\"], gig_data[\"delivery_times\"], gig_data[\"prices\"], gig_data[\"revisions\"], gig_data[\"tags\"], gig_data[\"language\"], gig_data[\"country\"], gig_data[\"member_since\"], gig_data[\"features\"]] )\n",
    "    with open('gigs.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open('gigs.txt', 'w') as f:\n",
    "        for line in lines:\n",
    "            if line.strip(\"\\n\") != gig_link:\n",
    "                f.write(line)\n",
    "\n",
    "driver.quit()\n",
    "# This function can make sure no process is left running in the background\n",
    "timeout_callback(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
